{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106f10f3",
   "metadata": {},
   "source": [
    "# Task 5: Quantum Graph Neural Network\n",
    "\n",
    "We will be extending the idea of Task 2 to classify quark/gluon using GNN, but this time we do this using quantum circuits\n",
    "\n",
    "The preprocessing part will be almost same.\n",
    "\n",
    "Let us first import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "from torch_cluster import knn_graph\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import TransformerConv\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2ed2c",
   "metadata": {},
   "source": [
    "## Load and Preprocess the dataset\n",
    "\n",
    "As done in task 2, since the data set is very large so we will load the dataset in batches \n",
    "\n",
    "For preprocessing , this time I will be deleting all the particle data that has all 0 values and rest of the data is normalized\n",
    "\n",
    "X : a 3D array of shape (num_jets, num_particles, num_features) where features are [pT, η, ϕ, PDG]. We keep only the first three columns.\n",
    "\n",
    "y: a 1D array of jet-level labels\n",
    "\n",
    "We will build a KNN graph \n",
    "\n",
    "Stores:<br>\n",
    "    x: tensor of normalized continuous features (shape: [num_nodes, 3])<br>\n",
    "    edge_index: graph connectivity<br>\n",
    "    y: jet-level label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d92656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(QGDataset, self).__init__(root, transform, pre_transform)\n",
    "        path = self.processed_paths[0]\n",
    "        if os.path.exists(path):\n",
    "            self.data, self.slices = torch.load(path)\n",
    "        else:\n",
    "            self.process()\n",
    "            self.data, self.slices = torch.load(path)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return glob.glob(os.path.join(self.raw_dir, '*.npz'))\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        batch_size = 5000  # Process in chunks to avoid memory overload\n",
    "        processed_path = self.processed_paths[0]\n",
    "\n",
    "        for fpath in tqdm(self.raw_file_names, desc=\"Processing NPZ Files\"):\n",
    "            npz = np.load(fpath, mmap_mode='r')\n",
    "            features = npz['X'].astype(np.float32)  \n",
    "            labels = npz['y'].astype(np.int64)\n",
    "            num_jets, num_particles, num_feats = features.shape\n",
    "\n",
    "            for i in range(num_jets):\n",
    "                x_np = features[i]\n",
    "                mask = ~np.all(x_np == 0, axis=1)\n",
    "                x_np = x_np[mask]\n",
    "                if x_np.size == 0:\n",
    "                    continue\n",
    "                x_cont_np = x_np[:, :3]\n",
    "                # Normalize features per jet\n",
    "                mean = np.mean(x_cont_np, axis=0, keepdims=True)\n",
    "                std = np.std(x_cont_np, axis=0, keepdims=True) + 1e-6\n",
    "                x_cont_np = (x_cont_np - mean) / std\n",
    "\n",
    "                x_cont = torch.tensor(x_cont_np, dtype=torch.float32)\n",
    "                y = torch.tensor([labels[i]], dtype=torch.long)\n",
    "                edge_index = knn_graph(x_cont, k=10, loop=False)\n",
    "\n",
    "                data_obj = Data(x=x_cont, edge_index=edge_index, y=y)\n",
    "                data_list.append(data_obj)\n",
    "\n",
    "                if len(data_list) >= batch_size:\n",
    "                    self._save_partial(data_list, processed_path)\n",
    "                    data_list = []\n",
    "        if data_list:\n",
    "            self._save_partial(data_list, processed_path)\n",
    "\n",
    "    def _save_partial(self, data_list, path):\n",
    "        if os.path.exists(path):\n",
    "            old_data, old_slices = torch.load(path)\n",
    "            if isinstance(old_data, Data):\n",
    "                old_data = [old_data]\n",
    "            data_list = list(old_data) + data_list\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        torch.save((self.data, self.slices), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9d3c2",
   "metadata": {},
   "source": [
    "## Quantum Circuit \n",
    "\n",
    "We will use 3 qubits to represent pT, η, ϕ \n",
    "\n",
    "`qml.AngleEmbedding`: Encodes input features as quantum angles.\n",
    "\n",
    "`qml.BasicEntanglerLayers`: Applies trainable entangling layers.\n",
    "\n",
    "`qml.expval(qml.PauliZ(i))`: Returns quantum measurement outputs.\n",
    "\n",
    "Wrap the quantum circuit in a PyTorch Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ce3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 3  \n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "        self.q_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8544057",
   "metadata": {},
   "source": [
    "## Hybrid Model\n",
    "\n",
    "1. Graph Convolution (TransformerConv)\n",
    "    Two layers:<br>\n",
    "    First layer: TransformerConv with multi-head attention (4 heads).<br>\n",
    "    Second layer: TransformerConv with single-head aggregation.<br>\n",
    "    These layers extract graph-based features from the jet data.<br>\n",
    "\n",
    "2. Global Pooling<br>\n",
    "    Applies `global_mean_pool()` to aggregate node-level features into jet-level representations.\n",
    "\n",
    "3. Quantum Feature Projection<br>\n",
    "    Projects features to a 3-dimensional space (matching the quantum circuit).<br>\n",
    "    Passes the features through the QuantumLayer.\n",
    "\n",
    "4. Final Classification Layer<br>\n",
    "    A linear layer maps quantum outputs to class logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e9131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridTransformerQuantumModel(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=64, out_dim=2, n_layers=3, n_qubits=3):\n",
    "        super(HybridTransformerQuantumModel, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = TransformerConv(in_channels, hidden_dim, heads=4, dropout=0.1)\n",
    "        # Layer 2\n",
    "        self.conv2 = TransformerConv(hidden_dim * 4, hidden_dim, heads=1, dropout=0.1)\n",
    "        # Projection layer to make output equal to qubits\n",
    "        self.feature_proj = nn.Linear(hidden_dim, n_qubits)\n",
    "        self.q_layer = QuantumLayer(n_qubits, n_layers)\n",
    "        self.lin = nn.Linear(n_qubits, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()   # Output shape: (num_nodes, hidden_dim*4)\n",
    "        x = self.conv2(x, edge_index).relu()   # Output shape: (num_nodes, hidden_dim)\n",
    "        x = global_mean_pool(x, batch)         # Graph-level pooling -> shape: (num_graphs, hidden_dim)\n",
    "        x = self.feature_proj(x)               # Project to (num_graphs, n_qubits=3)\n",
    "        x = self.q_layer(x)                    # Quantum layer -> shape: (num_graphs, n_qubits)\n",
    "        x = self.lin(x)                        # Final classification layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d71e4",
   "metadata": {},
   "source": [
    "## Train and Test Functions\n",
    "\n",
    "Same as Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d58412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in tqdm(loader, desc=\"Training\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5a6ea",
   "metadata": {},
   "source": [
    "## Main fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ad575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnav\\AppData\\Local\\Temp\\ipykernel_22224\\1247508449.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(path)\n",
      "C:\\Users\\arnav\\anaconda3\\envs\\qml\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 jets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 0.6887, Test Accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02, Loss: 0.6152, Test Accuracy: 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, Loss: 0.6001, Test Accuracy: 0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04, Loss: 0.5925, Test Accuracy: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05, Loss: 0.5853, Test Accuracy: 0.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06, Loss: 0.5811, Test Accuracy: 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07, Loss: 0.5787, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08, Loss: 0.5737, Test Accuracy: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09, Loss: 0.5699, Test Accuracy: 0.7140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.5672, Test Accuracy: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.5664, Test Accuracy: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.5673, Test Accuracy: 0.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.5665, Test Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.5878, Test Accuracy: 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.5630, Test Accuracy: 0.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.5715, Test Accuracy: 0.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.5619, Test Accuracy: 0.7310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.5619, Test Accuracy: 0.7140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.5572, Test Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.5584, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.5606, Test Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.5604, Test Accuracy: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.5630, Test Accuracy: 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.5602, Test Accuracy: 0.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.5569, Test Accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.5574, Test Accuracy: 0.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.5540, Test Accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.5571, Test Accuracy: 0.7320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.5619, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.5574, Test Accuracy: 0.7340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.5667, Test Accuracy: 0.7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.5892, Test Accuracy: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.5902, Test Accuracy: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.5727, Test Accuracy: 0.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.5787, Test Accuracy: 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.5541, Test Accuracy: 0.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.5494, Test Accuracy: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.5536, Test Accuracy: 0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.5519, Test Accuracy: 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.5585, Test Accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.5440, Test Accuracy: 0.7310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 0.5502, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 0.5589, Test Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 0.5492, Test Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:10<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 0.5544, Test Accuracy: 0.7310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.5506, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.5495, Test Accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 0.5494, Test Accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 0.5497, Test Accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 125/125 [00:09<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.5443, Test Accuracy: 0.7310\n",
      "Best Test Accuracy: 0.734\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    root_dir = 'qg_data'\n",
    "    dataset = QGDataset(root=root_dir)\n",
    "    print(f\"Processed {len(dataset)} jets.\")\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    test_dataset = dataset[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = QGNN(in_channels=3, hidden_dim=128, out_dim=2, n_layers=3, n_qubits=3).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(1, 51):\n",
    "        loss = train(model, train_loader, optimizer, device)\n",
    "        acc = test(model, test_loader, device)\n",
    "        print(f\"Epoch {epoch:02d}, Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"qgnn_improved_best.pth\")\n",
    "\n",
    "    print(\"Best Test Accuracy:\", best_acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e271eff",
   "metadata": {},
   "source": [
    "Due to memory issue I cannot increase its accuracy as by increasing any parameter or fine tuning leads to Out of Memory Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5edb2",
   "metadata": {},
   "source": [
    "This model gives accuracy of 73.4% which is less than the models we have done in task 2 because of the issues in Computational Power, maybe if I had more computational power, the quantum model would have performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a692033",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
