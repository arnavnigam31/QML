{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9311ed48",
   "metadata": {},
   "source": [
    "# Task 6 :Quantum Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0795d6",
   "metadata": {},
   "source": [
    "First we will import all the required libraries for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab836cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pennylane as qml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6841e51",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "The MNIST dataset has images 28x28 pixels<br>\n",
    "\n",
    "We can split the image in 4 parts for quantum encoding<br>\n",
    "\n",
    "Therefore we will be using 4 qubit for each image\n",
    "\n",
    "total_wires = 2 register of each qubit + one ancilla qubit for swap test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8953309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits_per_state = 4\n",
    "total_wires = 1 + 2 * n_qubits_per_state  \n",
    "dev = qml.device(\"default.qubit\", wires=total_wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c1c61",
   "metadata": {},
   "source": [
    "Seed initialization is used for random operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752c5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb084edd",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "\n",
    "Just split the image in 4 quadrants \n",
    "Return a tensor of this vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d4ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    img_np = img.numpy()\n",
    "    q1 = np.mean(img_np[:14, :14])\n",
    "    q2 = np.mean(img_np[:14, 14:])\n",
    "    q3 = np.mean(img_np[14:, :14])\n",
    "    q4 = np.mean(img_np[14:, 14:])\n",
    "    return torch.tensor([q1, q2, q3, q4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce26e0d",
   "metadata": {},
   "source": [
    "The encode_image function will encode the image by applying rotation gates\n",
    "\n",
    "Rotation angle is given by:<br>\n",
    "    θi =scale[i] × image[i] + bias[i]\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ae3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image, params, wires):\n",
    "    \n",
    "    for i, wire in enumerate(wires):\n",
    "        theta = params[i, 0] * image[i] + params[i, 1]\n",
    "        qml.RY(theta, wires=wire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5b20a",
   "metadata": {},
   "source": [
    "### Swap Circuit\n",
    "\n",
    "We will encode first image on wire 1 to 4 and second image on wire 5 to 8\n",
    "Both are encoded using same encoder and parameter\n",
    "\n",
    "The hamdard gate is applied to ancilla qubit ie q0 \n",
    "\n",
    "Then we apply Controlled Swap ie CSWAP on one qubit from each image\n",
    "\n",
    "Finally again Hamdard gate on ancilla \n",
    "\n",
    "The expectation value of the Pauli‑Z operator on the ancilla is measured. This value relates directly to the fidelity (similarity) between the two quantum states. If fidelity close to 1 means the states are similar, while a fidelity near 0 indicates dissimilarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a87706",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(image1, image2, params):\n",
    "    \n",
    "    encode_image(image1, params, wires=list(range(1, n_qubits_per_state + 1)))\n",
    "    \n",
    "    encode_image(image2, params, wires=list(range(n_qubits_per_state + 1, total_wires)))\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    for i in range(n_qubits_per_state):\n",
    "        qml.CSWAP(wires=[0, 1 + i, n_qubits_per_state + 1 + i])\n",
    "    qml.Hadamard(wires=0)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de0cda",
   "metadata": {},
   "source": [
    "Wrapping these above functionality in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875c8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantumNet, self).__init__()\n",
    "        self.params = torch.nn.Parameter(torch.randn(n_qubits_per_state, 2))\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "        proc_img1 = preprocess_image(img1)\n",
    "        proc_img2 = preprocess_image(img2)\n",
    "        \n",
    "        fidelity = quantum_circuit(proc_img1, proc_img2, self.params)\n",
    "        return fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c4624",
   "metadata": {},
   "source": [
    "Loss function\n",
    "\n",
    "Same Class (label = 1):\n",
    "The loss is (1−fidelity)^2\n",
    "Minimizing this loss encourages the fidelity to approach 1.\n",
    "\n",
    "\n",
    "Different Class (label = 0):\n",
    "The loss is fidility^2\n",
    "Minimizing this loss pushes the fidelity toward 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8d0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(fidelity, label):\n",
    "    # When label==1 (same class): loss = (1 - fidelity)^2\n",
    "    # When label==0 (different): loss = (fidelity)^2\n",
    "    return label * (1 - fidelity)**2 + (1 - label) * (fidelity)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf15269",
   "metadata": {},
   "source": [
    "Load the MNIST dataset\n",
    "\n",
    "First convert the image to pyTorch tensor\n",
    "\n",
    "Remove channel dimension using lambda fxn\n",
    "\n",
    "Due to memory constraint working only with first 1000 smaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712c8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.squeeze(0))#remove the channel dimension\n",
    "])\n",
    "mnist_train = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "data = [(mnist_train[i][0], mnist_train[i][1]) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b2ba6",
   "metadata": {},
   "source": [
    "## Final Training Loop \n",
    "\n",
    "An instance of `QuantumNet` is created, and an Adam optimizer is set up with a learning rate of **0.01** to update the trainable parameters.\n",
    "\n",
    "### Training Parameters\n",
    "The loop runs for **20 epochs**, with **200 iterations per epoch**.\n",
    "\n",
    "### Sampling Image Pairs\n",
    "In each iteration:\n",
    "- Two random indices are chosen to sample a pair from the dataset.\n",
    "- The corresponding images and labels are retrieved.\n",
    "- The target for the contrastive loss is set to **1.0** if the images have the same label, and **0.0** otherwise.\n",
    "\n",
    "- The **average loss per epoch** is printed, providing insight into the training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14eed1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8768, Accuracy: 7.50%\n",
      "Epoch 2, Loss: 0.7763, Accuracy: 10.50%\n",
      "Epoch 3, Loss: 0.7061, Accuracy: 9.00%\n",
      "Epoch 4, Loss: 0.6122, Accuracy: 13.50%\n",
      "Epoch 5, Loss: 0.4846, Accuracy: 22.00%\n",
      "Epoch 6, Loss: 0.4188, Accuracy: 31.50%\n",
      "Epoch 7, Loss: 0.3693, Accuracy: 37.00%\n",
      "Epoch 8, Loss: 0.3096, Accuracy: 52.50%\n",
      "Epoch 9, Loss: 0.3074, Accuracy: 51.50%\n",
      "Epoch 10, Loss: 0.2847, Accuracy: 56.50%\n",
      "Epoch 11, Loss: 0.2508, Accuracy: 59.00%\n",
      "Epoch 12, Loss: 0.2498, Accuracy: 57.00%\n",
      "Epoch 13, Loss: 0.2114, Accuracy: 69.00%\n",
      "Epoch 14, Loss: 0.2001, Accuracy: 68.00%\n",
      "Epoch 15, Loss: 0.2257, Accuracy: 64.00%\n",
      "Epoch 16, Loss: 0.1631, Accuracy: 75.50%\n",
      "Epoch 17, Loss: 0.1793, Accuracy: 73.00%\n",
      "Epoch 18, Loss: 0.1773, Accuracy: 74.50%\n",
      "Epoch 19, Loss: 0.1453, Accuracy: 78.00%\n",
      "Epoch 20, Loss: 0.1623, Accuracy: 77.50%\n",
      "Epoch 21, Loss: 0.1675, Accuracy: 74.00%\n",
      "Epoch 22, Loss: 0.1461, Accuracy: 78.50%\n",
      "Epoch 23, Loss: 0.1562, Accuracy: 77.50%\n",
      "Epoch 24, Loss: 0.1497, Accuracy: 80.00%\n",
      "Epoch 25, Loss: 0.1463, Accuracy: 77.00%\n",
      "Epoch 26, Loss: 0.1110, Accuracy: 86.00%\n",
      "Epoch 27, Loss: 0.1520, Accuracy: 77.50%\n",
      "Epoch 28, Loss: 0.1431, Accuracy: 81.00%\n",
      "Epoch 29, Loss: 0.1004, Accuracy: 90.50%\n",
      "Epoch 30, Loss: 0.1347, Accuracy: 81.00%\n",
      "Epoch 31, Loss: 0.1150, Accuracy: 85.00%\n",
      "Epoch 32, Loss: 0.1211, Accuracy: 83.50%\n",
      "Epoch 33, Loss: 0.1338, Accuracy: 83.50%\n",
      "Epoch 34, Loss: 0.1507, Accuracy: 78.50%\n",
      "Epoch 35, Loss: 0.1226, Accuracy: 84.00%\n",
      "Epoch 36, Loss: 0.0889, Accuracy: 87.00%\n",
      "Epoch 37, Loss: 0.1100, Accuracy: 84.50%\n",
      "Epoch 38, Loss: 0.1102, Accuracy: 83.00%\n",
      "Epoch 39, Loss: 0.1193, Accuracy: 85.00%\n",
      "Epoch 40, Loss: 0.1291, Accuracy: 83.00%\n"
     ]
    }
   ],
   "source": [
    "qnet = QuantumNet()\n",
    "optimizer = optim.Adam(qnet.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 40\n",
    "num_iterations = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0  # Track correct classifications\n",
    "    for _ in range(num_iterations):\n",
    "        idx1, idx2 = np.random.randint(0, len(data)), np.random.randint(0, len(data))\n",
    "        img1, label1 = data[idx1]\n",
    "        img2, label2 = data[idx2]\n",
    "        target = 1.0 if label1 == label2 else 0.0  # Ground truth label\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        fidelity = qnet(img1, img2)\n",
    "        loss = contrastive_loss(fidelity, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted_label = 1.0 if fidelity >= 0.5 else 0.0  # Classification threshold\n",
    "        if predicted_label == target:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    avg_loss = total_loss / num_iterations\n",
    "    accuracy = correct_predictions / num_iterations * 100  # Convert to percentage\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
