{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5be4c1",
   "metadata": {},
   "source": [
    "# Task 9:Kolmogorov-Arnold Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d1544",
   "metadata": {},
   "source": [
    "We will be training a KAN network using activation function B-spline on MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6704",
   "metadata": {},
   "source": [
    "We will start by importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4c5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb0ad9",
   "metadata": {},
   "source": [
    "## BSpline Activation Function\n",
    "\n",
    "This is a custom activation function\n",
    "\n",
    "It is used to smoothly transform the inputs \n",
    "\n",
    "`knots` are special points which are used to define the curve\n",
    "\n",
    "`coeff` are the values that adjust the shape of the curve\n",
    "\n",
    "`cubic_bspline` function the curve. It creates a smooth, non-linear transformation of input.\n",
    "\n",
    "`forward` function is like the activation function for neuron in neural network as it applies bspline fxn to each each input x and return the transformed output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22747230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSplineActivation(nn.Module):\n",
    "    def __init__(self, num_basis=5, knot_min=-2, knot_max=2):\n",
    "        super(BSplineActivation, self).__init__()\n",
    "        self.num_basis = num_basis\n",
    "        # Fixed knot locations, they need not be learned.\n",
    "        self.register_buffer('knots', torch.linspace(knot_min, knot_max, num_basis))\n",
    "        self.coeffs = nn.Parameter(torch.ones(num_basis))\n",
    "    \n",
    "    def cubic_bspline(self, x):\n",
    "        # Compute the cubic B-spline basis value.\n",
    "        abs_x = torch.abs(x)\n",
    "        val1 = (2/3) - (x**2) + 0.5 * (abs_x**3)\n",
    "        val2 = ((2 - abs_x)**3) / 6\n",
    "        return torch.where(abs_x < 1, val1,torch.where((abs_x >= 1) & (abs_x < 2), val2, torch.zeros_like(x)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #  compute the B-spline value\n",
    "        out = 0\n",
    "        for i in range(self.num_basis):\n",
    "            # calculate (x - knot).\n",
    "            b_val = self.cubic_bspline(x - self.knots[i])\n",
    "            out = out + self.coeffs[i] * b_val\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af412fd4",
   "metadata": {},
   "source": [
    "## KAN Model\n",
    "\n",
    "We will define the neural network model\n",
    "\n",
    "An input layer `fc1`: Converts 28x28 images into a long vector.\n",
    "\n",
    "A hidden layer `spline`: Uses the B-Spline activation.\n",
    "\n",
    "An output layer `fc2`: Gives 10 output values (one for each digit 0-9).\n",
    "\n",
    "\n",
    "`Forward`\n",
    "The image is flattened (28x28 â†’ 784).\n",
    "\n",
    "A linear transformation (fc1)\n",
    "\n",
    "The B-Spline activation function\n",
    "\n",
    "Another linear transformation (fc2)\n",
    "\n",
    "The final output is 10 values, representing predictions for digits 0-9.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, num_basis=5, num_classes=10):\n",
    "        super(KANNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.spline = BSplineActivation(num_basis=num_basis)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten MNIST images (28x28 -> 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        # Apply the spline activation (nonlinear univariate function)\n",
    "        x = self.spline(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1253121",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Transformations are applied:\n",
    "\n",
    "Convert images to tensors.\n",
    "\n",
    "Normalize pixel values for better learning.\n",
    "\n",
    "Data is divided into batches of 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0101ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d15ee",
   "metadata": {},
   "source": [
    "set model, use adam optimizer and cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17645853",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KANNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b2016",
   "metadata": {},
   "source": [
    "Training and Testing fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e653c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss   = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}]  Loss: {loss.item():.6f}\")\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91488bed",
   "metadata": {},
   "source": [
    "Main fxn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2074b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 [0/60000]  Loss: 2.456431\n",
      "Train Epoch 1 [6400/60000]  Loss: 0.356771\n",
      "Train Epoch 1 [12800/60000]  Loss: 0.325261\n",
      "Train Epoch 1 [19200/60000]  Loss: 0.128849\n",
      "Train Epoch 1 [25600/60000]  Loss: 0.338014\n",
      "Train Epoch 1 [32000/60000]  Loss: 0.179312\n",
      "Train Epoch 1 [38400/60000]  Loss: 0.116476\n",
      "Train Epoch 1 [44800/60000]  Loss: 0.098487\n",
      "Train Epoch 1 [51200/60000]  Loss: 0.175551\n",
      "Train Epoch 1 [57600/60000]  Loss: 0.074753\n",
      "Test set: Average loss: 0.1197, Accuracy: 9650/10000 (96.50%)\n",
      "Train Epoch 2 [0/60000]  Loss: 0.124895\n",
      "Train Epoch 2 [6400/60000]  Loss: 0.142201\n",
      "Train Epoch 2 [12800/60000]  Loss: 0.186275\n",
      "Train Epoch 2 [19200/60000]  Loss: 0.106183\n",
      "Train Epoch 2 [25600/60000]  Loss: 0.094119\n",
      "Train Epoch 2 [32000/60000]  Loss: 0.025040\n",
      "Train Epoch 2 [38400/60000]  Loss: 0.175447\n",
      "Train Epoch 2 [44800/60000]  Loss: 0.067399\n",
      "Train Epoch 2 [51200/60000]  Loss: 0.050091\n",
      "Train Epoch 2 [57600/60000]  Loss: 0.072920\n",
      "Test set: Average loss: 0.0889, Accuracy: 9723/10000 (97.23%)\n",
      "Train Epoch 3 [0/60000]  Loss: 0.064827\n",
      "Train Epoch 3 [6400/60000]  Loss: 0.048971\n",
      "Train Epoch 3 [12800/60000]  Loss: 0.033758\n",
      "Train Epoch 3 [19200/60000]  Loss: 0.083907\n",
      "Train Epoch 3 [25600/60000]  Loss: 0.179422\n",
      "Train Epoch 3 [32000/60000]  Loss: 0.044245\n",
      "Train Epoch 3 [38400/60000]  Loss: 0.086158\n",
      "Train Epoch 3 [44800/60000]  Loss: 0.025418\n",
      "Train Epoch 3 [51200/60000]  Loss: 0.020873\n",
      "Train Epoch 3 [57600/60000]  Loss: 0.015032\n",
      "Test set: Average loss: 0.0951, Accuracy: 9697/10000 (96.97%)\n",
      "Train Epoch 4 [0/60000]  Loss: 0.032289\n",
      "Train Epoch 4 [6400/60000]  Loss: 0.034286\n",
      "Train Epoch 4 [12800/60000]  Loss: 0.043628\n",
      "Train Epoch 4 [19200/60000]  Loss: 0.009331\n",
      "Train Epoch 4 [25600/60000]  Loss: 0.023568\n",
      "Train Epoch 4 [32000/60000]  Loss: 0.093715\n",
      "Train Epoch 4 [38400/60000]  Loss: 0.093260\n",
      "Train Epoch 4 [44800/60000]  Loss: 0.055522\n",
      "Train Epoch 4 [51200/60000]  Loss: 0.084395\n",
      "Train Epoch 4 [57600/60000]  Loss: 0.021980\n",
      "Test set: Average loss: 0.0932, Accuracy: 9717/10000 (97.17%)\n",
      "Train Epoch 5 [0/60000]  Loss: 0.050172\n",
      "Train Epoch 5 [6400/60000]  Loss: 0.060684\n",
      "Train Epoch 5 [12800/60000]  Loss: 0.111625\n",
      "Train Epoch 5 [19200/60000]  Loss: 0.129068\n",
      "Train Epoch 5 [25600/60000]  Loss: 0.030935\n",
      "Train Epoch 5 [32000/60000]  Loss: 0.025470\n",
      "Train Epoch 5 [38400/60000]  Loss: 0.092779\n",
      "Train Epoch 5 [44800/60000]  Loss: 0.088442\n",
      "Train Epoch 5 [51200/60000]  Loss: 0.077101\n",
      "Train Epoch 5 [57600/60000]  Loss: 0.032946\n",
      "Test set: Average loss: 0.0955, Accuracy: 9699/10000 (96.99%)\n",
      "Train Epoch 6 [0/60000]  Loss: 0.019628\n",
      "Train Epoch 6 [6400/60000]  Loss: 0.029890\n",
      "Train Epoch 6 [12800/60000]  Loss: 0.029617\n",
      "Train Epoch 6 [19200/60000]  Loss: 0.131167\n",
      "Train Epoch 6 [25600/60000]  Loss: 0.031547\n",
      "Train Epoch 6 [32000/60000]  Loss: 0.132707\n",
      "Train Epoch 6 [38400/60000]  Loss: 0.020274\n",
      "Train Epoch 6 [44800/60000]  Loss: 0.064796\n",
      "Train Epoch 6 [51200/60000]  Loss: 0.038765\n",
      "Train Epoch 6 [57600/60000]  Loss: 0.141061\n",
      "Test set: Average loss: 0.0974, Accuracy: 9706/10000 (97.06%)\n",
      "Train Epoch 7 [0/60000]  Loss: 0.036692\n",
      "Train Epoch 7 [6400/60000]  Loss: 0.027578\n",
      "Train Epoch 7 [12800/60000]  Loss: 0.040076\n",
      "Train Epoch 7 [19200/60000]  Loss: 0.021739\n",
      "Train Epoch 7 [25600/60000]  Loss: 0.070987\n",
      "Train Epoch 7 [32000/60000]  Loss: 0.035413\n",
      "Train Epoch 7 [38400/60000]  Loss: 0.078171\n",
      "Train Epoch 7 [44800/60000]  Loss: 0.028670\n",
      "Train Epoch 7 [51200/60000]  Loss: 0.034445\n",
      "Train Epoch 7 [57600/60000]  Loss: 0.068992\n",
      "Test set: Average loss: 0.1165, Accuracy: 9635/10000 (96.35%)\n",
      "Train Epoch 8 [0/60000]  Loss: 0.081017\n",
      "Train Epoch 8 [6400/60000]  Loss: 0.023539\n",
      "Train Epoch 8 [12800/60000]  Loss: 0.056131\n",
      "Train Epoch 8 [19200/60000]  Loss: 0.066282\n",
      "Train Epoch 8 [25600/60000]  Loss: 0.038217\n",
      "Train Epoch 8 [32000/60000]  Loss: 0.059021\n",
      "Train Epoch 8 [38400/60000]  Loss: 0.121459\n",
      "Train Epoch 8 [44800/60000]  Loss: 0.033824\n",
      "Train Epoch 8 [51200/60000]  Loss: 0.037281\n",
      "Train Epoch 8 [57600/60000]  Loss: 0.012281\n",
      "Test set: Average loss: 0.1010, Accuracy: 9705/10000 (97.05%)\n",
      "Train Epoch 9 [0/60000]  Loss: 0.014294\n",
      "Train Epoch 9 [6400/60000]  Loss: 0.022332\n",
      "Train Epoch 9 [12800/60000]  Loss: 0.050132\n",
      "Train Epoch 9 [19200/60000]  Loss: 0.042760\n",
      "Train Epoch 9 [25600/60000]  Loss: 0.033948\n",
      "Train Epoch 9 [32000/60000]  Loss: 0.034645\n",
      "Train Epoch 9 [38400/60000]  Loss: 0.071121\n",
      "Train Epoch 9 [44800/60000]  Loss: 0.022558\n",
      "Train Epoch 9 [51200/60000]  Loss: 0.041229\n",
      "Train Epoch 9 [57600/60000]  Loss: 0.124367\n",
      "Test set: Average loss: 0.1102, Accuracy: 9672/10000 (96.72%)\n",
      "Train Epoch 10 [0/60000]  Loss: 0.004530\n",
      "Train Epoch 10 [6400/60000]  Loss: 0.043787\n",
      "Train Epoch 10 [12800/60000]  Loss: 0.044037\n",
      "Train Epoch 10 [19200/60000]  Loss: 0.194782\n",
      "Train Epoch 10 [25600/60000]  Loss: 0.117504\n",
      "Train Epoch 10 [32000/60000]  Loss: 0.039180\n",
      "Train Epoch 10 [38400/60000]  Loss: 0.032302\n",
      "Train Epoch 10 [44800/60000]  Loss: 0.024973\n",
      "Train Epoch 10 [51200/60000]  Loss: 0.081640\n",
      "Train Epoch 10 [57600/60000]  Loss: 0.015299\n",
      "Test set: Average loss: 0.1128, Accuracy: 9654/10000 (96.54%)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1868a",
   "metadata": {},
   "source": [
    "The best accuracy is 97.23 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8651e49",
   "metadata": {},
   "source": [
    "# Quantum KAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52952d01",
   "metadata": {},
   "source": [
    "We can extend the classical KAN to quantum KAN to use quantum properties like superposition and entanglement that will make the execution fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d10db7",
   "metadata": {},
   "source": [
    "## Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da40db",
   "metadata": {},
   "source": [
    "1. Encoding \n",
    "The input of pixel can be encoded to quantum state using angle encoding or amplitude encoding \n",
    "\n",
    "2. Linear Transformation \n",
    "\n",
    "3. Quantum Activation function\n",
    "Instead of using Bspline, we can use quantum activation fxn like VQC, QFT etc.\n",
    "\n",
    "4. Convert the quantum layer output to classical output and then map the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be36bf",
   "metadata": {},
   "source": [
    "                Classical Input (Flattened 784 MNIST pixels)\n",
    "                              |\n",
    "                              V\n",
    "      +-----------------------------------------------------+\n",
    "      |  Linear Transformation (Classical Fully Connected)  |\n",
    "      +-----------------------------------------------------+\n",
    "                              |\n",
    "                              V\n",
    "      +---------------------------------------------------+\n",
    "      | Quantum Feature Encoding (Map Classical to Qubits)| \n",
    "      | Example: Angle Encoding:  Î¸ = Ï€ * normalized(x)   |\n",
    "      +---------------------------------------------------+\n",
    "                              |\n",
    "                              V\n",
    "      +-------------------------------------------------------+\n",
    "      | Quantum Activation Function (Parameterized Circuit)   |\n",
    "      | - Qubits undergo transformations via quantum gates    |\n",
    "      | - Nonlinearity introduced through quantum interference|\n",
    "      | - Measurement extracts classical feature values       |\n",
    "      +-------------------------------------------------------+\n",
    "                              |\n",
    "                              V\n",
    "      +-----------------------------------------------------+\n",
    "      |  Fully Connected Layer (Classical Readout)          |\n",
    "      |  - Maps quantum features to hidden neurons          |\n",
    "      +-----------------------------------------------------+\n",
    "                              |\n",
    "                              V\n",
    "      +----------------------------------+\n",
    "      |  Output Layer (Classical)        |\n",
    "      |  - 10 neurons for digit classes  |\n",
    "      +----------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b8f50",
   "metadata": {},
   "source": [
    "## Advantages of QKAN over KAN\n",
    "\n",
    "Quantum entanglement allows good fxn approximations\n",
    "\n",
    "Few parameter are needed \n",
    "\n",
    "Speed Increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d537b96-e8fe-4b57-8e63-94adc8385b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 [0/60000]  Loss: 2.442454\n",
      "Train Epoch 1 [6400/60000]  Loss: 2.376703\n",
      "Train Epoch 1 [12800/60000]  Loss: 2.305700\n",
      "Train Epoch 1 [19200/60000]  Loss: 2.292043\n",
      "Train Epoch 1 [25600/60000]  Loss: 2.310628\n",
      "Train Epoch 1 [32000/60000]  Loss: 2.311791\n",
      "Train Epoch 1 [38400/60000]  Loss: 2.303499\n",
      "Train Epoch 1 [44800/60000]  Loss: 2.283513\n",
      "Train Epoch 1 [51200/60000]  Loss: 2.281255\n",
      "Train Epoch 1 [57600/60000]  Loss: 2.294804\n",
      "Test set: Average loss: 2.2967, Accuracy: 1066/10000 (10.66%)\n",
      "Train Epoch 2 [0/60000]  Loss: 2.283395\n",
      "Train Epoch 2 [6400/60000]  Loss: 2.289648\n",
      "Train Epoch 2 [12800/60000]  Loss: 2.301621\n",
      "Train Epoch 2 [19200/60000]  Loss: 2.304470\n",
      "Train Epoch 2 [25600/60000]  Loss: 2.292690\n",
      "Train Epoch 2 [32000/60000]  Loss: 2.278935\n",
      "Train Epoch 2 [38400/60000]  Loss: 2.286330\n",
      "Train Epoch 2 [44800/60000]  Loss: 2.302229\n",
      "Train Epoch 2 [51200/60000]  Loss: 2.279379\n",
      "Train Epoch 2 [57600/60000]  Loss: 2.284999\n",
      "Test set: Average loss: 2.2911, Accuracy: 1100/10000 (11.00%)\n",
      "Train Epoch 3 [0/60000]  Loss: 2.283823\n",
      "Train Epoch 3 [6400/60000]  Loss: 2.277641\n",
      "Train Epoch 3 [12800/60000]  Loss: 2.293749\n",
      "Train Epoch 3 [19200/60000]  Loss: 2.284212\n",
      "Train Epoch 3 [25600/60000]  Loss: 2.270528\n",
      "Train Epoch 3 [32000/60000]  Loss: 2.294852\n",
      "Train Epoch 3 [38400/60000]  Loss: 2.293701\n",
      "Train Epoch 3 [44800/60000]  Loss: 2.305026\n",
      "Train Epoch 3 [51200/60000]  Loss: 2.291771\n",
      "Train Epoch 3 [57600/60000]  Loss: 2.302829\n",
      "Test set: Average loss: 2.2856, Accuracy: 1343/10000 (13.43%)\n",
      "Train Epoch 4 [0/60000]  Loss: 2.297766\n",
      "Train Epoch 4 [6400/60000]  Loss: 2.293232\n",
      "Train Epoch 4 [12800/60000]  Loss: 2.291041\n",
      "Train Epoch 4 [19200/60000]  Loss: 2.280107\n",
      "Train Epoch 4 [25600/60000]  Loss: 2.270379\n",
      "Train Epoch 4 [32000/60000]  Loss: 2.283283\n",
      "Train Epoch 4 [38400/60000]  Loss: 2.270339\n",
      "Train Epoch 4 [44800/60000]  Loss: 2.274036\n",
      "Train Epoch 4 [51200/60000]  Loss: 2.293747\n",
      "Train Epoch 4 [57600/60000]  Loss: 2.281500\n",
      "Test set: Average loss: 2.2804, Accuracy: 1509/10000 (15.09%)\n",
      "Train Epoch 5 [0/60000]  Loss: 2.274093\n",
      "Train Epoch 5 [6400/60000]  Loss: 2.272843\n",
      "Train Epoch 5 [12800/60000]  Loss: 2.285500\n",
      "Train Epoch 5 [19200/60000]  Loss: 2.268864\n",
      "Train Epoch 5 [25600/60000]  Loss: 2.271374\n",
      "Train Epoch 5 [32000/60000]  Loss: 2.283595\n",
      "Train Epoch 5 [38400/60000]  Loss: 2.263728\n",
      "Train Epoch 5 [44800/60000]  Loss: 2.282680\n",
      "Train Epoch 5 [51200/60000]  Loss: 2.271480\n",
      "Train Epoch 5 [57600/60000]  Loss: 2.267489\n",
      "Test set: Average loss: 2.2755, Accuracy: 1580/10000 (15.80%)\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# using 2 qubits\n",
    "dev = qml.device(\"default.qubit\", wires=2)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_activation(inputs, weights):\n",
    "    qml.RX(inputs[0], wires=0)  \n",
    "    qml.RX(inputs[1], wires=1)  \n",
    "    qml.CNOT(wires=[0, 1])      \n",
    "    qml.RZ(weights[0], wires=0) \n",
    "    qml.RZ(weights[1], wires=1)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(2)]  \n",
    "\n",
    "class QKAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QKAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 2)\n",
    "        self.q_weights = nn.Parameter(torch.rand(2))\n",
    "        self.fc2 = nn.Linear(2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        q_out = torch.stack([\n",
    "            torch.tensor(quantum_activation(x[i].to(\"cpu\"), self.q_weights.to(\"cpu\")), dtype=torch.float32).to(device)\n",
    "            for i in range(x.shape[0])\n",
    "        ])\n",
    "        x = self.fc2(q_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Load MNIST Dataset\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, download=True, transform=transform), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize Model, Loss Function, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QKAN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)  #flatten images\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}]  Loss: {loss.item():.6f}\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)  \n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")\n",
    "    return accuracy\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db52a4",
   "metadata": {},
   "source": [
    "This is a sample code of implementation QKAN, we can redefine model and fine tune it to get better accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
