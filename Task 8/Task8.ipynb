{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9abfa38",
   "metadata": {},
   "source": [
    "# Task 8 : Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52870a77",
   "metadata": {},
   "source": [
    "We will start by importinh all the requires libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ebcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29349f9",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "Multi-Head Self-Attention: Helps the model focus on different parts of the image.\n",
    "\n",
    "Layer Normalization: Normalizes data for stable learning.\n",
    "\n",
    "MLP (Feedforward Neural Network): Learns deep representations.\n",
    "\n",
    "Residual Connections: Adds previous layer's output back to avoid loss of information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f21833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape:(seq_len,batch_size,embed_dim)\n",
    "        x2 = self.norm1(x)\n",
    "        attn_output, _ = self.attn(x2, x2, x2)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.mlp(x2)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476aec5c",
   "metadata": {},
   "source": [
    "## Vision Transformer\n",
    "\n",
    "Patch Embedding Layer: Converts an image into small patches using Conv2D.\n",
    "\n",
    "Class Token: A special token that helps in classification.\n",
    "\n",
    "Positional Embeddings: Helps the model understand patch positions.\n",
    "\n",
    "Transformer Blocks: A sequence of TransformerBlock layers.\n",
    "\n",
    "Normalization & Head Layer: Normalizes the data and applies a final linear layer to classify digits (0-9).\n",
    "\n",
    "### Forward pass\n",
    "Extract Patches → Flatten → Embed.\n",
    "\n",
    "Add Class Token.\n",
    "\n",
    "Apply Transformer Blocks.\n",
    "\n",
    "Extract Class Token Output and pass it through a fully connected layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec41be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=7, in_channels=1, num_classes=10,\n",
    "                 embed_dim=64, depth=6, num_heads=4, mlp_dim=128, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding using a convolution (acts as patch extractor and linear projection)\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # token and positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # parameters initialzised\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        # x: (B, in_channels, image_size, image_size)\n",
    "        x = self.patch_embed(x)  # shape: (B, embed_dim, H', W')\n",
    "        x = x.flatten(2).transpose(1, 2)  # shape: (B, num_patches, embed_dim)\n",
    "        \n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # shape: (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # shape: (B, num_patches+1, embed_dim)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer expects input shape (sequence_length, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Use class token output for classification\n",
    "        cls_output = x[0]  # shape: (B, embed_dim)\n",
    "        logits = self.head(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef627a7",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806ef4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06008e7",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f505044",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685a944",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a071eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6251, Test Accuracy = 92.80%\n",
      "Epoch 2: Train Loss = 0.2130, Test Accuracy = 95.74%\n",
      "Epoch 3: Train Loss = 0.1655, Test Accuracy = 96.28%\n",
      "Epoch 4: Train Loss = 0.1385, Test Accuracy = 96.79%\n",
      "Epoch 5: Train Loss = 0.1200, Test Accuracy = 97.04%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {loss:.4f}, Test Accuracy = {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95654b4",
   "metadata": {},
   "source": [
    "# Quantum Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e149f5b",
   "metadata": {},
   "source": [
    "QVT is a model that provides transformer architecture with quantum computing\n",
    "\n",
    "## Architecture \n",
    "\n",
    "### 1. Input and Patch Embedding (Classical)\n",
    "- Input: MNIST image of size 28×28.\n",
    "- Patch Extraction: Divide the image into **7×7 patches** (16 patches in total).\n",
    "- Linear Projection: Map each patch (**flattened 49 pixels**) into a higher-dimensional embedding (e.g., 64 dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Quantum Encoding Layer\n",
    "- Encoding Circuit:\n",
    "  - For each patch embedding vector $$x \\in \\mathbb{R}^{64} $$, use a quantum circuit where each element of the vector controls rotation gates (e.g.,  R_y( theta)  gates).  \n",
    "  - This encodes the classical data into a quantum state.\n",
    "\n",
    "- Quantum Register: \n",
    "  - Use a set of qubits sufficient to represent the embedding.\n",
    "  - Techniques like qubit re-use or amplitude encoding can be used to reduce qubit count.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quantum Transformer Block\n",
    "\n",
    "### Quantum Self-Attention\n",
    "- Quantum Circuits for Q, K, V: \n",
    "  - Create separate parameterized quantum circuits that transform the encoded patch state into quantum representations of queries (Q), keys (K), and values (V).\n",
    "- Attention via Quantum Similarity:\n",
    "  - Implement a subroutine (e.g., **swap test**) that calculates the **similarity (overlap)** between the quantum states of queries and keys.\n",
    "  - These similarity scores act as the **attention weights**.\n",
    "\n",
    "### Quantum Feed-Forward (Variational Circuit)\n",
    "- Pass the weighted quantum states through another parameterized quantum circuit that mimics the function of a classical MLP.\n",
    "- Hybrid Update: Use measurements to convert the quantum information back to **classical data before feeding it to the next block (or to a classical MLP).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classification Head (Classical)\n",
    "- Aggregate the processed information from the class token (or a pooled representation of the patch outputs).\n",
    "- Final Linear Layer: Feed the classical vector into a fully connected layer to obtain class logits for digit classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Considerations\n",
    "- Hybrid Optimization: Train the entire network using a mix of classical backpropagation and quantum-specific gradient techniques (e.g., parameter shift rule)\n",
    "- Resource Constraints:\n",
    "  - Given current quantum hardware limitations the quantum circuits should be kept shallow and use few qubits\n",
    "  - Simulation-based experiments on classical hardware can be used during the prototyping phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085954e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
