{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Quantum Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing quantum classifier using Cirq.\n",
    "\n",
    "Since the data is in npz file and the cirq library supports python version 3.9 so, first we will save the file using a lower protocol by using python version 3.12 then we can use our standard 3.9 version which we are using for all other task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shapes: (50, 5) (50, 5)\n",
      "Original testing data shapes: (50, 5) (50, 5)\n",
      "Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "\n",
    "#Load the original container file\n",
    "container = np.load('task4data.npz', allow_pickle=True)\n",
    "\n",
    "#Extract the inner files \n",
    "train_data = container['training_input.npy']\n",
    "test_data  = container['test_input.npy']\n",
    "\n",
    "#check if they are 0D array it means it is a dictionary \n",
    "if isinstance(train_data, np.ndarray) and train_data.ndim == 0:\n",
    "    train_data = train_data.item()\n",
    "if isinstance(test_data, np.ndarray) and test_data.ndim == 0:\n",
    "    test_data = test_data.item()\n",
    "\n",
    "X_train = train_data['0']\n",
    "y_train = train_data['1']\n",
    "X_test  = test_data['0']\n",
    "y_test  = test_data['1']\n",
    "# just check the shape of data\n",
    "print(\"Original training data shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Original testing data shapes:\", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Save the files separatedly with protocol that support 3.9 version of python\n",
    "np.save('training_input_converted.npy', train_data, allow_pickle=True)\n",
    "np.save('test_input_converted.npy', test_data, allow_pickle=True)\n",
    "\n",
    "\n",
    "# create the new container file as the dataset was before\n",
    "converted_train = np.load('training_input_converted.npy', allow_pickle=True)\n",
    "converted_test  = np.load('test_input_converted.npy', allow_pickle=True)\n",
    "\n",
    "np.savez('task4data_converted.npz',training_input=converted_train,test_input=converted_test)\n",
    "\n",
    "print(\"Conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can switch to our original environment and of version 3.9\n",
    "\n",
    "Import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cirq\n",
    "import sympy\n",
    "import copy\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the converted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: (50, 5) (50, 5)\n",
      "Testing data shapes: (50, 5) (50, 5)\n"
     ]
    }
   ],
   "source": [
    "container = np.load('task4data_converted.npz', allow_pickle=True)\n",
    "train_data = container['training_input']\n",
    "test_data  = container['test_input']\n",
    "\n",
    "if isinstance(train_data, np.ndarray) and train_data.ndim == 0:\n",
    "    train_data = train_data.item()\n",
    "if isinstance(test_data, np.ndarray) and test_data.ndim == 0:\n",
    "    test_data = test_data.item()\n",
    "\n",
    "# Data keys: '0' for features and '1' for labels.\n",
    "X_train = train_data['0']\n",
    "y_train = train_data['1']\n",
    "X_test  = test_data['0']\n",
    "y_test  = test_data['1']\n",
    "\n",
    "print(\"Training data shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data and convert label to scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test  = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "y_train_scalar = np.array([int(y[0]) if isinstance(y, np.ndarray) and len(y) > 1 else int(y) for y in y_train])\n",
    "y_test_scalar  = np.array([int(y[0]) if isinstance(y, np.ndarray) and len(y) > 1 else int(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Encoding \n",
    "\n",
    "We will be encoding each sample point using Rx rotation and then  apply parameterized rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_circuit(features, params):\n",
    "    num_features = len(features)\n",
    "    qubits = cirq.GridQubit.rect(1, num_features)\n",
    "    circuit = cirq.Circuit()\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        circuit.append(cirq.rx(float(feature))(qubits[i]))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        circuit.append(cirq.rx(float(params[i]))(qubits[i]))\n",
    "    \n",
    "    return circuit, qubits\n",
    "\n",
    "simulator = cirq.Simulator()\n",
    "\n",
    "def circuit_expectation(features, params):\n",
    "    circuit, qubits = create_classifier_circuit(features, params)\n",
    "    observable = cirq.Z(qubits[0])\n",
    "    result = simulator.simulate_expectation_values(circuit, observables=[observable])\n",
    "    return result[0].real\n",
    "\n",
    "# map probability in 0 to 1 range\n",
    "def predict_probability(features, params):\n",
    "    exp_val = circuit_expectation(features, params)\n",
    "    return (1 + exp_val) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use binary cross entropy and compute gradient using parameter shift rule\n",
    "\n",
    "### **Mathematical Formulation of Gradient Computation**\n",
    "\n",
    "#### **1. Binary Cross-Entropy Loss**\n",
    "$$\n",
    "\\mathcal{L} = - y \\log(p) - (1 - y) \\log(1 - p)\n",
    "$$\n",
    "where \\( p \\) is the predicted probability.\n",
    "\n",
    "#### **2. Probability Computation from Expectation Value**\n",
    "$$\n",
    "p = \\frac{1 + \\langle Z \\rangle}{2}\n",
    "$$\n",
    "\n",
    "#### **3. Gradient of Loss w.r.t. \\( p \\)**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p} = -\\frac{y}{p} + \\frac{(1 - y)}{(1 - p)}\n",
    "$$\n",
    "\n",
    "#### **4. Parameter-Shift Rule for Quantum Gradients**\n",
    "Using the **parameter-shift rule**, the gradient of the expectation value is computed as:\n",
    "$$\n",
    "\\frac{\\partial \\langle Z \\rangle}{\\partial \\theta_j} = \\frac{\\langle Z \\rangle (\\theta_j + \\frac{\\pi}{2}) - \\langle Z \\rangle (\\theta_j - \\frac{\\pi}{2})}{2}\n",
    "$$\n",
    "\n",
    "#### **5. Applying the Chain Rule**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\frac{\\partial \\mathcal{L}}{\\partial p} \\times \\frac{1}{2} \\times \\frac{\\partial \\langle Z \\rangle}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "#### **6. Final Gradient Update**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\left(-\\frac{y}{p} + \\frac{(1 - y)}{(1 - p)}\\right) \\times \\frac{1}{2} \\times \\frac{\\langle Z \\rangle (\\theta_j + \\frac{\\pi}{2}) - \\langle Z \\rangle (\\theta_j - \\frac{\\pi}{2})}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\left(-\\frac{y}{p} + \\frac{(1 - y)}{(1 - p)}\\right) \\times \\frac{\\langle Z \\rangle (\\theta_j + \\frac{\\pi}{2}) - \\langle Z \\rangle (\\theta_j - \\frac{\\pi}{2})}{4}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_true, p):\n",
    "    eps = 1e-8\n",
    "    return - (y_true * np.log(p + eps) + (1 - y_true) * np.log(1 - p + eps))\n",
    "\n",
    "def loss_for_sample(features, y_true, params):\n",
    "    p = predict_probability(features, params)\n",
    "    return binary_cross_entropy_loss(y_true, p)\n",
    "\n",
    "def compute_gradients(features, y_true, params):\n",
    "    grad = np.zeros_like(params)\n",
    "    p = predict_probability(features, params)\n",
    "    dL_dp = - y_true / p + (1 - y_true) / (1 - p)\n",
    "    \n",
    "    shift = math.pi / 2\n",
    "    for j in range(len(params)):\n",
    "        params_plus = copy.deepcopy(params)\n",
    "        params_minus = copy.deepcopy(params)\n",
    "        params_plus[j] += shift\n",
    "        params_minus[j] -= shift\n",
    "        \n",
    "        exp_plus = circuit_expectation(features, params_plus)\n",
    "        exp_minus = circuit_expectation(features, params_minus)\n",
    "        d_exp = (exp_plus - exp_minus) / 2  # parameter-shift derivative\n",
    "        \n",
    "        grad[j] = dL_dp * d_exp / 4  # combined chain rule factor\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training quantum classifier...\n",
      "Epoch 1/20 - Loss: 4.9188 | Train Acc: 0.7800 | Test Acc: 0.4400\n",
      "Epoch 2/20 - Loss: 0.9971 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 3/20 - Loss: 0.9380 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 4/20 - Loss: 0.8976 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 5/20 - Loss: 0.8683 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 6/20 - Loss: 0.8463 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 7/20 - Loss: 0.8297 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 8/20 - Loss: 0.8171 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 9/20 - Loss: 0.8075 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 10/20 - Loss: 0.8003 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 11/20 - Loss: 0.7949 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 12/20 - Loss: 0.7909 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 13/20 - Loss: 0.7880 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 14/20 - Loss: 0.7859 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 15/20 - Loss: 0.7844 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 16/20 - Loss: 0.7833 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 17/20 - Loss: 0.7826 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 18/20 - Loss: 0.7821 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 19/20 - Loss: 0.7817 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "Epoch 20/20 - Loss: 0.7815 | Train Acc: 0.7800 | Test Acc: 0.8200\n",
      "\n",
      "Final Evaluation:\n",
      "Test Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "num_params = X_train.shape[1]  # One parameter per feature (qubit)\n",
    "params = np.random.randn(num_params)  # Initialize parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 20\n",
    "\n",
    "print(\"\\nTraining quantum classifier...\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_grad = np.zeros_like(params)\n",
    "    \n",
    "    # Loop over each training sample.\n",
    "    for i in range(len(X_train)):\n",
    "        x_sample = X_train[i]\n",
    "        y_sample = y_train_scalar[i]\n",
    "        loss_val = loss_for_sample(x_sample, y_sample, params)\n",
    "        grad_val = compute_gradients(x_sample, y_sample, params)\n",
    "        total_loss += loss_val\n",
    "        total_grad += grad_val\n",
    "        \n",
    "    avg_loss = total_loss / len(X_train)\n",
    "    avg_grad = total_grad / len(X_train)\n",
    "    \n",
    "    # Update parameters using gradient descent.\n",
    "    params -= learning_rate * avg_grad\n",
    "    \n",
    "    # training accuracy.\n",
    "    train_preds = [1 if predict_probability(x, params) >= 0.5 else 0 for x in X_train]\n",
    "    train_acc = np.mean(train_preds == y_train_scalar)\n",
    "    \n",
    "    # test metrics.\n",
    "    test_probs = np.array([predict_probability(x, params) for x in X_test])\n",
    "    test_preds = (test_probs >= 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_scalar, test_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "test_probs = np.array([predict_probability(x, params) for x in X_test])\n",
    "test_preds = (test_probs >= 0.5).astype(int)\n",
    "test_acc = accuracy_score(y_test_scalar, test_preds)\n",
    "\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy is 82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are initializing the parameters randomly the accuracy changes each time we run the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
